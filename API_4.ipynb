{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helloisachinaide/ProjetoAPI4/blob/main/API_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGzXxR-hcz8l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import glob\n",
        "\n",
        "# Defina o caminho do arquivo .txt\n",
        "txt_file = '2023Carga.txt'\n",
        "\n",
        "# Ler o arquivo .txt e exibir os primeiros registros para verificar a leitura\n",
        "df = pd.read_csv(txt_file, delimiter=';', encoding='utf-8')\n",
        "\n",
        "# Exibir as primeiras linhas do DataFrame para verificar se foi lido corretamente\n",
        "print(\"Visualização dos dados lidos do arquivo .txt:\")\n",
        "print(df.head())\n",
        "\n",
        "# Salvar o arquivo como .csv, caso o DataFrame não esteja vazio\n",
        "if not df.empty:\n",
        "    df.to_csv('2023Carga.csv', encoding='utf-8', index=False)\n",
        "    print(\"Arquivo convertido de .txt para .csv com sucesso!\")\n",
        "else:\n",
        "    print(\"Erro: O DataFrame está vazio após a leitura do arquivo .txt\")\n",
        "\n",
        "# Verificar se o CSV gerado está correto\n",
        "df_check = pd.read_csv(txt_file, encoding='utf-8')\n",
        "print(\"Visualização do arquivo CSV gerado:\")\n",
        "print(df_check.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Excluir dados desnecessarios\n",
        "\n",
        "df = pd.read_csv('2023Carga.csv')\n",
        "\n",
        "# Excluir colunas indesejadas\n",
        "colunas_a_excluir = ['Carga Geral Acondicionamento', 'ConteinerEstado', 'FlagAutorizacao', 'FlagCabotagem', 'FlagCabotagemMovimentacao', 'FlagConteinerTamanho', 'FlagLongoCurso', 'FlagMCOperacaoCarga', 'FlagOffshore', 'FlagTransporteViaInterioir']  # Substitua pelos nomes das colunas que você deseja excluir\n",
        "df = df.drop(columns=colunas_a_excluir)\n",
        "\n",
        "# Exibir as colunas após a exclusão\n",
        "print(\"Colunas após a exclusão:\", df.columns)\n",
        "\n",
        "# Salvar o DataFrame modificado de volta em um novo arquivo CSV\n",
        "df.to_csv('2023Carga_NOVO.csv', index=False)"
      ],
      "metadata": {
        "id": "HpexKHOodX66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#excluir dados nulos\n",
        "\n",
        "df = pd.read_csv('2023Atracacao.csv')\n",
        "\n",
        "# Excluir linhas onde 'Data Chegada' é nula\n",
        "df = df.dropna(subset=['Data Chegada'])\n",
        "\n",
        "# Exibir as primeiras linhas após a exclusão\n",
        "print(\"Após a exclusão:\")\n",
        "print(df.head())\n",
        "\n",
        "# Salvar o DataFrame modificado em um novo arquivo CSV\n",
        "df.to_csv('2023Atracacao_mod.csv', index=False)"
      ],
      "metadata": {
        "id": "uPO_3JZ6dcsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Juntar todas as tabelas de atracacao e carga de 2014-2023 em dois CSV's\n",
        "\n",
        "\n",
        "# Encontrar todos os arquivos .csv no diretório atual\n",
        "arquivos = glob.glob('*.csv')\n",
        "\n",
        "# Criando uma lista para armazenar os DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Lendo apenas os arquivos de atracação (mod) ou carga(Carga)\n",
        "for arquivo in arquivos:\n",
        "    if 'mod' in arquivo:  # Verifica se o nome contém 'Carga' ou 'mod'\n",
        "        try:\n",
        "            df = pd.read_csv(arquivo)\n",
        "            dataframes.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao ler {arquivo}: {e}\")\n",
        "\n",
        "# Verifica se há DataFrames para concatenar\n",
        "if dataframes:\n",
        "    # Concatenando todos os DataFrames de atracação ou carga\n",
        "    resultado = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "    # Salvando o resultado em um novo arquivo .csv\n",
        "    resultado.to_csv('arquivo_atracacao_final_NOVO.csv', index=False)\n",
        "    print(\"Arquivos de atracação unidos com sucesso!\")\n",
        "else:\n",
        "\n",
        "    print(\"Nenhum arquivo de atracação válido encontrado.\")"
      ],
      "metadata": {
        "id": "M9CdgmHzdfjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filtrar a tabela de TODAS as cargas para somente SH4 1005 = milho\n",
        "\n",
        "\n",
        "data = pd.read_csv('arquivo_carga_final.csv')\n",
        "\n",
        "\n",
        "# Filtrar apenas os registros do SH4 1005\n",
        "filtered_data = data[data['CDMercadoria'] == '1005']\n",
        "\n",
        "# Salvar os dados filtrados em um novo arquivo CSV\n",
        "filtered_data.to_csv('arquivo_carga_final_(so_milho).csv', index=False)"
      ],
      "metadata": {
        "id": "0d5QO-ffdjVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converte datas para novo padrão\n",
        "\n",
        "ANTAQ_Limpa = pd.read_csv('arquivo_atracacao_final_NOVO.csv', low_memory=False)\n",
        "\n",
        "colunas_data = [\n",
        "    'Data Chegada', 'Data Atracação', 'Data Início Operação',\n",
        "    'Data Término Operação', 'Data Desatracação'\n",
        "]\n",
        "\n",
        "for coluna in colunas_data:\n",
        "    ANTAQ_Limpa[coluna] = pd.to_datetime(ANTAQ_Limpa[coluna], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
        "\n",
        "# As colunas agora estão no formato datetime\n",
        "print(\"Colunas de data convertidas com sucesso.\")\n",
        "ANTAQ_Limpa.to_csv('arquivo_atracacao_final_NOVO.csv', index=False)"
      ],
      "metadata": {
        "id": "CI0SNbGFeRRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#criaçao de novas colunas fazendo calculos pertinentes\n",
        "\n",
        "ANTAQ_Limpa['Tempo_espera_chegada'] = (ANTAQ_Limpa['Data Atracação'] - ANTAQ_Limpa['Data Chegada']).dt.total_seconds() / 3600\n",
        "ANTAQ_Limpa['Tempo_espera_inicial'] = (ANTAQ_Limpa['Data Início Operação'] - ANTAQ_Limpa['Data Atracação']).dt.total_seconds() / 3600\n",
        "ANTAQ_Limpa['Tempo_operacao'] = (ANTAQ_Limpa['Data Término Operação'] - ANTAQ_Limpa['Data Início Operação']).dt.total_seconds() / 3600\n",
        "ANTAQ_Limpa['Tempo_espera_final'] = (ANTAQ_Limpa['Data Desatracação'] - ANTAQ_Limpa['Data Término Operação']).dt.total_seconds() / 3600\n",
        "ANTAQ_Limpa['Tempo_ocupacao'] = (ANTAQ_Limpa['Data Desatracação'] - ANTAQ_Limpa['Data Atracação']).dt.total_seconds() / 3600\n",
        "\n",
        "print(\"Colunas novas criadas com sucesso.\")\n",
        "ANTAQ_Limpa.to_csv('arquivo_atracacao_final_NOVO.csv', index=False)"
      ],
      "metadata": {
        "id": "ltqsHliEeSy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenaçao dos dados, juntando tudo em um CSV\n",
        "\n",
        "tabela1 = pd.read_csv('arquivo_atracacao_final_NOVO.csv', low_memory=False)\n",
        "tabela2 = pd.read_csv('arquivo_carga_final_(so_milho).csv', low_memory=False)\n",
        "\n",
        "resultado = pd.merge(tabela1, tabela2, on='IDAtracacao', how='inner')\n",
        "\n",
        "pd.DataFrame(resultado)\n",
        "resultado.to_csv('carga_&_atracacao(so_milho)_NOVO.csv', index=False)\n",
        "print('dados juntados')\n",
        "print(resultado)"
      ],
      "metadata": {
        "id": "OZ5eLWDoeVXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conferencia se ha dados duplicados depois de contatenar, se tiver são excluidos\n",
        "\n",
        "ANTAQ_Limpa = pd.read_csv('carga_&_atracacao(so_milho).csv', low_memory=False)\n",
        "\n",
        "# Identificar duplicados\n",
        "duplicados = ANTAQ_Limpa['IDAtracacao'].duplicated(keep=False)\n",
        "\n",
        "# Excluir todas as linhas que têm valores duplicados\n",
        "ANTAQ_Limpa = ANTAQ_Limpa[~duplicados]\n",
        "\n",
        "# Exibir o DataFrame atualizado\n",
        "print(\"Tabela atualizada sem duplicatas na coluna IDAtracacao:\")\n",
        "print(ANTAQ_Limpa)\n",
        "\n",
        "\n",
        "\n",
        "ANTAQ_Limpa.to_csv('carga_&_atracacao_NOVO.csv', index=False)"
      ],
      "metadata": {
        "id": "x7Gu6W3WeZWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#arquivo final: carga_&_atracacao_NOVO.csv"
      ],
      "metadata": {
        "id": "WABcueclecXO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}